{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of assignment aims to provide you with some exposure to the use of autoencoders. Use\n",
    "the full MNIST dataset for this problem.\n",
    "Hints: Use corruption level = 0.1, training epochs = up to about 25, learning rate = 0.1, and batch\n",
    "size = 128 for training of all the layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For an easier implementation we would like to implement most of the function in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "\n",
    "def init_weights(n_in, n_out, name_weight):\n",
    "    weight = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_in + n_out)),\n",
    "            high=4 * np.sqrt(6. / (n_in + n_out)),\n",
    "            size=(n_in, n_out)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=weight, name=name_weight, borrow=True)\n",
    "\n",
    "\n",
    "def init_bias(n, name_bias):\n",
    "    return theano.shared(value=np.zeros(n, dtype=theano.config.floatX), name=name_bias, borrow=True)\n",
    "\n",
    "\n",
    "def init_weight_biases_4dimension(filter_shape, d_type):\n",
    "    fan_in = np.prod(filter_shape[1:])\n",
    "    fan_out = filter_shape[0] * np.prod(filter_shape[2:])\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[0],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def init_weight_biases_2dimensions(filter_shape, d_type):\n",
    "    fan_in = filter_shape[1]\n",
    "    fan_out = filter_shape[0]\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[1],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def shuffle_data(samples, labels):\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAutoEncoder:\n",
    "    def __init__(self, num_features, num_outputs, list_hidden_layer, learning_rate,\n",
    "                 corruption_level=0.01,\n",
    "                 sparsity_parameter=0.01, penalty_parameter=0.01):\n",
    "\n",
    "        \"\"\"\n",
    "        :param list_hidden_layer: [10, 20] -> means 2 hidden layer, 10 neurons ->layer_1, 10 neurons -> layer_2\n",
    "        :param corruption_level: made the noise in the data\n",
    "        :param learning_rate: learning rate\n",
    "        :param sparsity_parameter: 0.1 -> means 0.1 of neurons is activated\n",
    "        :param penalty_parameter: learning decay\n",
    "        \"\"\"\n",
    "\n",
    "        list_neurons = [num_features] + list_hidden_layer\n",
    "        x = T.matrix('x')\n",
    "        d = T.matrix('d')\n",
    "\n",
    "        self.total_costs_auto_encoder = []\n",
    "        self.total_costs_full = []\n",
    "        self.total_predictions_full = []\n",
    "\n",
    "        \"\"\"\n",
    "            Do the data corruption\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.corrupt_the_data(corruption_level=corruption_level, x=x)\n",
    "\n",
    "        \"\"\"\n",
    "            Construct the auto encoder\n",
    "        \"\"\"\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        prev_output = x\n",
    "\n",
    "        \"\"\"\n",
    "            ENCODER\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(1, len(list_neurons)):\n",
    "\n",
    "            weight = init_weights(list_neurons[i-1], list_neurons[i], 'weight_%s' % i)\n",
    "            bias = init_bias(list_neurons[i], 'bias_%s' % i)\n",
    "\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "\n",
    "            prev_output = T.nnet.sigmoid(T.dot(prev_output, weight) + bias)\n",
    "\n",
    "        \"\"\"\n",
    "            DECODER\n",
    "        \"\"\"\n",
    "\n",
    "        buffer_output = prev_output\n",
    "        biases_trans = []\n",
    "\n",
    "        for i in range(len(weights)-1, -1, -1):\n",
    "\n",
    "            weight_transpose = weights[i].transpose()\n",
    "            bias_transpose = init_bias(list_neurons[i], 'bias_trans_%s' % i)\n",
    "\n",
    "            biases_trans.append(bias_transpose)\n",
    "\n",
    "            prev_output = T.nnet.sigmoid(T.dot(prev_output, weight_transpose) + bias_transpose)\n",
    "\n",
    "        cost = - T.mean(T.sum(x * T.log(prev_output) + (1 - x) * T.log(1 - prev_output), axis=1))\n",
    "\n",
    "        params = weights+biases+biases_trans\n",
    "        grads = T.grad(cost, params)\n",
    "        updates = [(param, param - learning_rate*grad) for param, grad in zip(params, grads)]\n",
    "\n",
    "        self.train_encoder = theano.function(\n",
    "            inputs=[x],\n",
    "            updates=updates,\n",
    "            outputs=[prev_output, cost]\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "            TRAIN THE FULL CONNECTED LAYER\n",
    "        \"\"\"\n",
    "\n",
    "        last_weight = init_weights(list_neurons[-1], num_outputs, 'last_weight')\n",
    "        last_bias = init_bias(num_outputs, 'last_bias')\n",
    "\n",
    "        buffer_output = T.nnet.softmax(T.dot(buffer_output, last_weight) + last_bias)\n",
    "        y_pred = T.argmax(buffer_output, axis=1)\n",
    "\n",
    "        cost_cross = T.mean(T.nnet.categorical_crossentropy(buffer_output, d))\n",
    "\n",
    "        params_full = weights + [last_weight] + biases + [last_bias]\n",
    "        grads_full = T.grad(cost_cross, params_full)\n",
    "        updates_full = [(param, param - learning_rate * grad ) for param, grad in zip(params_full, grads_full)]\n",
    "\n",
    "        self.train_cross = theano.function(\n",
    "            inputs=[x, d],\n",
    "            updates=updates_full,\n",
    "            outputs=[prev_output, y_pred, cost_cross]\n",
    "        )\n",
    "\n",
    "        self.test_cross = theano.function(\n",
    "            inputs=[x],\n",
    "            outputs=[y_pred]\n",
    "        )\n",
    "\n",
    "    def corrupt_the_data(self, corruption_level, x):\n",
    "\n",
    "        # use binomial dist at corrupt the data\n",
    "        rng = np.random.RandomState(123)\n",
    "        theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "        tilde_x = theano_rng.binomial(size=x.shape, n=1, p=1 - corruption_level,\n",
    "                                      dtype=theano.config.floatX) * x\n",
    "\n",
    "        return tilde_x\n",
    "\n",
    "    def start_train_auto_encoder(self, epochs, batch_size, train_x, train_y, verbose=False):\n",
    "\n",
    "        print \"Start training the auto encoder\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # go through trainng set\n",
    "\n",
    "            costs = []\n",
    "            results = []\n",
    "\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_y), batch_size)):\n",
    "                result, cost = self.train_encoder(train_x[start:end])\n",
    "                costs.append(cost)\n",
    "                results.append(result)\n",
    "\n",
    "            self.total_costs_auto_encoder.append(np.mean(costs, dtype='float64'))\n",
    "\n",
    "            if epoch % 100 == 0 and verbose:\n",
    "                print \"Epoch: %d Cost: %s \\n\" % (epoch, self.total_costs_auto_encoder[epoch])\n",
    "\n",
    "    def start_train_the_full(self, epochs, batch_size, train_x, train_y, test_x, test_y):\n",
    "\n",
    "        print \"Start training the full hidden layer with autoencoder\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # go through trainng set\n",
    "            costs = []\n",
    "            results = []\n",
    "\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_y), batch_size)):\n",
    "                output, result, cost = self.train_cross(train_x[start:end], train_y[start:end])\n",
    "                costs.append(cost)\n",
    "                results.append(np.mean(np.argmax(test_y, axis=1) == self.test_cross(test_x)))\n",
    "\n",
    "            self.total_costs_full.append(np.mean(costs, dtype='float64'))\n",
    "            self.total_predictions_full.append(np.mean(results, dtype='float64'))\n",
    "            print \"result: %s, cost: %s \\n\" % (self.total_costs_full[epoch], self.total_predictions_full[epoch])\n",
    "\n",
    "    def get_total_costs_of_auto_encoder(self):\n",
    "\n",
    "        return self.total_costs_auto_encoder\n",
    "\n",
    "    def get_total_cost_and_prediction_full(self):\n",
    "\n",
    "        return self.total_costs_full, self.total_predictions_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collections class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "class DataCollector:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "\n",
    "        with open(file_path, \"rb\") as input_file:\n",
    "            self.data = cPickle.load(input_file)\n",
    "\n",
    "        self.data_train = self.data[0]\n",
    "        self.data_test = self.data[2]\n",
    "        self.data_validate = self.data[1]\n",
    "\n",
    "    def get_train_data(self):\n",
    "\n",
    "        return self.data_train[0], self.return_one_hot_encoding(10, self.data_train[1])\n",
    "\n",
    "    def get_test_data(self):\n",
    "\n",
    "        return self.data_test[0], self.return_one_hot_encoding(10, self.data_test[1])\n",
    "\n",
    "    def get_validation_data(self):\n",
    "\n",
    "        return self.data_validate[0], self.return_one_hot_encoding(10, self.data_validate[1])\n",
    "\n",
    "    def return_one_hot_encoding(self, num_output, list_data):\n",
    "\n",
    "        zeros = np.zeros((len(list_data), num_output))\n",
    "\n",
    "        for i in range(len(zeros)):\n",
    "\n",
    "            zeros[i][list_data[i]] = 1\n",
    "\n",
    "        return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataVisualization:\n",
    "\n",
    "   def __init__(self):\n",
    "       return\n",
    "\n",
    "   def show_plot(self, list_x_point, list_y_point, x_label, y_label, title, figure_name):\n",
    "       plt.figure()\n",
    "       plt.plot(list_x_point, list_y_point)\n",
    "       plt.xlabel(x_label)\n",
    "       plt.ylabel(y_label)\n",
    "       plt.title(title)\n",
    "       plt.savefig(figure_name)\n",
    "       plt.show()\n",
    "\n",
    "\n",
    "class DataVisualizationWithLabels:\n",
    "   def __init__(self):\n",
    "       return\n",
    "\n",
    "   def show_plot(self, list_x_point, list_y_point, x_label, y_label, title, figure_name, labels):\n",
    "       plt.figure()\n",
    "\n",
    "       for cnt in range(len(labels)):\n",
    "           plt.plot(list_x_point[cnt], list_y_point[cnt], label=labels[cnt])\n",
    "\n",
    "       plt.xlabel(x_label)\n",
    "       plt.ylabel(y_label)\n",
    "       plt.title(title)\n",
    "       plt.legend()\n",
    "       plt.savefig(figure_name)\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector(\"../mnist.pkl\")\n",
    "data_visualize = DataVisualizationWithLabels()\n",
    "\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "validate_x, validate_y = data_collector.get_validation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Design a stacked denoising autoencoder consisting of three hidden-layers; 900 neurons in the\n",
    "first hidden-layer, 625 neurons in the second hidden-layer, and 400 neurons in the third\n",
    "hidden-layer. To train the network:\n",
    "- Use the training dataset of MNIST digits\n",
    "- Corrupt the input data using a binomial distribution at 10% corruption level.\n",
    "- Use cross-entropy as the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the auto encoder\n",
      "Epoch: 0 Cost: 540.394155405 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112027810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_feature = len(train_x[0])\n",
    "epochs = 10\n",
    "batch_size = 20000\n",
    "file_save = './images/1_cost.png'\n",
    "\n",
    "softmax = SoftmaxAutoEncoder(num_features=num_feature, num_outputs=10,\n",
    "                             list_hidden_layer=[10], learning_rate=0.05,\n",
    "                            corruption_level=0.01)\n",
    "\n",
    "softmax.start_train_auto_encoder(epochs=epochs, batch_size=batch_size, train_x=train_x, \n",
    "                                 train_y=train_y, verbose=True)\n",
    "\n",
    "total_costs_auto_encoder = softmax.get_total_costs_of_auto_encoder()\n",
    "\n",
    "\n",
    "data_visualize.show_plot(\n",
    "    list_y_point=total_costs_auto_encoder, \n",
    "    list_x_point=range(epochs),\n",
    "    x_label='epochs',\n",
    "    y_label='costs',\n",
    "    title='cross entropy costs',\n",
    "    figure_name=file_save,\n",
    "    labels=['cost']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
