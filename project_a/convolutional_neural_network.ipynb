{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part A: Deep Convolutional Neural Network\n",
    "\n",
    "This project serves as assingment 2 of CZ4042 Neural Network (Nanyang Technological University).\n",
    "\n",
    "This part of the assignment aims to give you exposure to the use of convolutional neural networks (CNN) for object recognition in images.\n",
    "\n",
    "The project uses hand-written digit images provided by the MNIST database.\n",
    "Use only a subset of MNIST data for training and testing and select first 12,000 images from MNIST train data as training dataset and first 2000 of MNIST test data as testing dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "To understand the code better, the Convolutional Neural network will be implemented as a class. It give us flexibility in trying different configuration for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n",
    "\n",
    "Additional library used for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Softmax Class\n",
    "\n",
    "- Flexible layer initialization\n",
    "- SGD learning (with and without momentum)\n",
    "- RMSProp learning\n",
    "- ReLu activation function\n",
    "- Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Convolutional Neural Network with Softmax output function \"\"\"\n",
    "\n",
    "class SoftmaxCNN:\n",
    "    \"\"\" Softmax CNN Class \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv_layers=[]\n",
    "        self.pool_layers=[]\n",
    "        self.hidden_layers=[]\n",
    "        \n",
    "        self.train = None\n",
    "        self.predict = None\n",
    "        self.test = None\n",
    "        \n",
    "        self.learning = None\n",
    "        self.learning_rate = None\n",
    "        self.decay = None\n",
    "\n",
    "        # For SGD learning\n",
    "        self.momentum = None\n",
    "\n",
    "        # For RMSProp\n",
    "        self.rho = None\n",
    "        self.epsilon = None\n",
    "\n",
    "\n",
    "    def init_learning_sgd(self, learning_rate=0.05, decay=0.0001, momentum=0.0):\n",
    "        \"\"\" Initialise parameter for SGD learning \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.learning = self.sgd\n",
    "        if momentum > 0.0:\n",
    "            self.learning = self.sgd_momentum\n",
    "\n",
    "\n",
    "    def init_learning_RMSProp(self, learning_rate=0.001, decay=0.0001, rho=0.9, epsilon=1e-6):\n",
    "        \"\"\" Initialise parameter for RMSProp learning \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.learning = self.RMSprop\n",
    "\n",
    "\n",
    "    def create_model(self, convolutional_layer, pooling_layer, hidden_layer, num_output):\n",
    "        \"\"\"\n",
    "        :param convolutional_layer: list of convolutional [(15,1,9,9), (20,1,5,5)] -> first conv 15 kernels\n",
    "                                    with size 9x9\n",
    "        :param pooling_layer: list of ppoling layer [(2,2), (2,2)] -> first pool downsizing the cinv by 4x4\n",
    "                                Notes that the the length of conv and pool layer need to be same\n",
    "        :param hidden_layer: List of normal full connected layer [100]\n",
    "        \"\"\"\n",
    "        theano.exception_verbosity='high'\n",
    "\n",
    "        x = T.tensor4('x')\n",
    "        d = T.matrix('d')\n",
    "\n",
    "        weights_conv = []\n",
    "        biases_conv = []\n",
    "\n",
    "        # size conv and pool is strictly same\n",
    "        size_conv_pool = len(convolutional_layer)\n",
    "\n",
    "        prev_output = x\n",
    "\n",
    "        \"\"\"\n",
    "            Construct the Convolutional together with the Pooling Layer\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(0, size_conv_pool, 1):\n",
    "            weight_conv, bias_conv = init_weight_biases_4dimension(\n",
    "                convolutional_layer[i], x.dtype)\n",
    "\n",
    "            conv_out = T.nnet.relu(T.nnet.conv2d(prev_output, weight_conv) + bias_conv.dimshuffle('x', 0, 'x', 'x'))\n",
    "            prev_output = pool.pool_2d(conv_out, pooling_layer[i])\n",
    "\n",
    "            weights_conv.append(weight_conv)\n",
    "            biases_conv.append(bias_conv)\n",
    "            self.conv_layers.append(conv_out)\n",
    "            self.pool_layers.append(prev_output)\n",
    "\n",
    "        \"\"\"\n",
    "            Construct the full connected layer\n",
    "        \"\"\"\n",
    "        prev_output = T.flatten(prev_output, outdim=2)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        for i in range(0, len(hidden_layer), 1):\n",
    "\n",
    "            weight, bias = init_weight_biases_2dimensions(hidden_layer[i], x.dtype)\n",
    "\n",
    "            prev_output = T.nnet.relu(T.dot(prev_output, weight) + bias)\n",
    "\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "            self.hidden_layers.append(prev_output)\n",
    "\n",
    "        weight, bias = init_weight_biases_2dimensions((hidden_layer[-1][1], num_output), x.dtype)\n",
    "\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "\n",
    "        prev_output = T.nnet.softmax(T.dot(prev_output, weight) + bias)\n",
    "\n",
    "        prediction = T.argmax(prev_output, axis=1)\n",
    "\n",
    "        cost = T.mean(T.nnet.categorical_crossentropy(prev_output, d))\n",
    "        params = weights_conv + biases_conv + weights + biases\n",
    "\n",
    "        updates = self.learning(cost, params)\n",
    "\n",
    "        outputs = [cost] + self.conv_layers + self.pool_layers + self.hidden_layers\n",
    "        \n",
    "        self.train = theano.function(inputs=[x, d], outputs=outputs, updates=updates, allow_input_downcast=True)\n",
    "        self.predict = theano.function(inputs=[x], outputs=prediction, allow_input_downcast=True)\n",
    "        self.test = theano.function(inputs=[x], outputs=outputs[1:], allow_input_downcast=True)\n",
    "\n",
    "\n",
    "    def sgd(self, cost, params):\n",
    "        \"\"\" Learning using SGD \"\"\"\n",
    "        grads = T.grad(cost=cost, wrt=params)\n",
    "        updates = []\n",
    "        for p, g in zip(params, grads):\n",
    "            updates.append([p, p - (g + self.decay * p) * self.learning_rate])\n",
    "        return updates\n",
    "\n",
    "\n",
    "    def sgd_momentum(self, cost, params):\n",
    "        \"\"\" Learning using SGD with momentum \"\"\"\n",
    "        grads = T.grad(cost=cost, wrt=params)\n",
    "        updates = []\n",
    "        for p, g in zip(params, grads):\n",
    "            v = theano.shared(p.get_value()*0.)\n",
    "            v_new = self.momentum*v - (g + self.decay*p) * self.learning_rate\n",
    "            updates.append([p, p + v_new])\n",
    "            updates.append([v, v_new])\n",
    "        \n",
    "        return updates\n",
    "\n",
    "\n",
    "    def RMSprop(self, cost, params):\n",
    "        \"\"\" Learning using RMSProp \"\"\"\n",
    "        grads = T.grad(cost=cost, wrt=params)\n",
    "        updates = []\n",
    "        for p, g in zip(params, grads):\n",
    "            acc = theano.shared(p.get_value() * 0.)\n",
    "            acc_new = self.rho * acc + (1 - self.rho) * g ** 2\n",
    "            gradient_scaling = T.sqrt(acc_new + self.epsilon)\n",
    "            g = g / gradient_scaling\n",
    "            updates.append((acc, acc_new))\n",
    "            updates.append((p, p - self.learning_rate * (g+ self.decay*p)))\n",
    "        return updates\n",
    "\n",
    "\n",
    "    def start_train(self, tr_x, tr_y, te_x, te_y, epochs, batch_size):\n",
    "        \"\"\" Start training based on initialized model \"\"\"\n",
    "        self.predictions = []\n",
    "        self.costs = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            tr_x, tr_y = shuffle_data(tr_x, tr_y)\n",
    "            te_x, te_y = shuffle_data(te_x, te_y)\n",
    "\n",
    "            costs = []\n",
    "\n",
    "            for start, end in zip(range(0, len(tr_x), batch_size), range(batch_size, len(tr_y), batch_size)):\n",
    "                outputs = self.train(tr_x[start:end], tr_y[start:end])\n",
    "                costs.append(outputs[0])\n",
    "\n",
    "            self.predictions.append(np.mean(np.argmax(te_y, axis=1) == self.predict(te_x)))\n",
    "            self.costs.append(np.mean(costs))\n",
    "\n",
    "            print('epoch: %d, accuracy: %s, cost: %s \\n' % (i+1, self.predictions[i], self.costs[i]))\n",
    "        \n",
    "        return self.predictions, self.costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Colector\n",
    "\n",
    "Helper to get the train and test data from mnist.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Data collector for mnist.pkl \n",
    "\"\"\"\n",
    "\n",
    "class DataCollector:\n",
    "    \"\"\" DataCollector class \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, \"rb\") as input_file:\n",
    "            self.data = pickle.load(input_file, encoding=\"latin-1\")\n",
    "\n",
    "        self.data_train = self.data[0]\n",
    "        self.data_test = self.data[2]\n",
    "        self.data_validate = self.data[1]\n",
    "\n",
    "    def get_train_data(self):\n",
    "        \"\"\" get train data with one hot encoding \"\"\"\n",
    "        return self.data_train[0], self.return_one_hot_encoding(10, self.data_train[1])\n",
    "\n",
    "    def get_test_data(self):\n",
    "        \"\"\" get test data with one hot encoding \"\"\"\n",
    "        return self.data_test[0], self.return_one_hot_encoding(10, self.data_test[1])\n",
    "\n",
    "    def get_validation_data(self):\n",
    "        \"\"\" get validation data with one hot encoding \"\"\"\n",
    "        return self.data_validate[0], self.return_one_hot_encoding(10, self.data_validate[1])\n",
    "\n",
    "    def return_one_hot_encoding(self, num_output, list_data):\n",
    "        \"\"\" One hot encoding \"\"\"\n",
    "        zeros = np.zeros((len(list_data), num_output))\n",
    "\n",
    "        for i in range(len(zeros)):\n",
    "            zeros[i][list_data[i]] = 1\n",
    "\n",
    "        return zeros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Visualize the data as graphs or image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Data Visualization for CNN Softmax \"\"\"\n",
    "\n",
    "class DataVisualization:\n",
    "    \"\"\"\n",
    "    Data Visualizer\n",
    "    - Graph visualizer\n",
    "    - Image visualizer\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self):\n",
    "       return\n",
    "\n",
    "    def plot_graphs(self, list_x_point, list_y_point, x_label, y_label, title, figure_name=\"\", show_image=True):\n",
    "        \"\"\" Plot graphs from data\"\"\"\n",
    "        plt.figure()\n",
    "        plt.plot(list_x_point, list_y_point)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        \n",
    "        if figure_name != \"\":\n",
    "            plt.savefig(figure_name)\n",
    "        \n",
    "        if show_image:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def plot_images(self, data, figure_name=\"\", number_column=10, limit_image=20, size=28, transpose=True, show_image=True):\n",
    "        \"\"\" Plot images from data\"\"\"\n",
    "        width = height = size\n",
    "\n",
    "        plt.figure()\n",
    "        plt.gray()\n",
    "        row = limit_image/number_column\n",
    "\n",
    "        for i in range(1, limit_image+1):\n",
    "            plt.subplot(row, number_column, i)\n",
    "            plt.axis('off')\n",
    "            if transpose:\n",
    "                norm_image = self.normalize_image(data[:, i-1].reshape(width, height))\n",
    "            else:\n",
    "                norm_image = self.normalize_image(data[i-1].reshape(width, height))\n",
    "            plt.imshow(norm_image)\n",
    "\n",
    "        if figure_name != \"\":\n",
    "            plt.savefig(figure_name)\n",
    "        \n",
    "        if show_image:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        \"\"\" Normalize image data \"\"\"\n",
    "        min_val = np.min(image)\n",
    "        max_val = np.max(image)\n",
    "\n",
    "        return (image-min_val)/(max_val-min_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility\n",
    "\n",
    "Additional utility functions used in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Utility function for CNN Softmax\n",
    "- Weights and biases 4d initialization\n",
    "- Weights and biases 2d initialization\n",
    "- Shufle data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def init_weight_biases_4dimension(filter_shape, d_type):\n",
    "    \"\"\" Initialized 4d weights and biases \"\"\"\n",
    "    fan_in = np.prod(filter_shape[1:])\n",
    "    fan_out = filter_shape[0] * np.prod(filter_shape[2:])\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[0],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def init_weight_biases_2dimensions(filter_shape, d_type):\n",
    "    \"\"\" Initialized 2d weights and biases \"\"\"\n",
    "    fan_in = filter_shape[1]\n",
    "    fan_out = filter_shape[0]\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[1],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def shuffle_data(samples, labels):\n",
    "    \"\"\" Shuffle data \"\"\"\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "To recognize MNIST digits, design a convolutional neural network consisting of\n",
    "\n",
    "- An Input layer of 28x28 dimensions\n",
    "- A convolution layer C1 of 15 feature maps and filters of window size 9x9. A max pooling layer S1 with a pooling window of size 2x2.\n",
    "- A convolution layer C2 of 20 feature maps and filters of window size 5x5. A max pooling layer S2 with a pooling window of size 2x2.\n",
    "- A fully connected layer F3 of size 100.\n",
    "- A softmax layer F4 of size 10.\n",
    "\n",
    "Train the network using ReLu activation functions for neurons and mini batch gradient descent learning. Set batch size 128, learning rate (alpha) = 0.05 and decay parameter (beta) = 10−4. \n",
    "\n",
    "- Plot the training cost and test accuracy with learning epochs.\n",
    "- For two representative test patterns, plot the feature maps at the convolution and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Start script question 1\n",
    "\"\"\" \n",
    "\n",
    "# import numpy as np\n",
    "# from project_a.cnn_softmax import SoftmaxCNN\n",
    "# from project_a.data_collector import DataCollector\n",
    "# from project_a.data_visualization import DataVisualization\n",
    "\n",
    "def question1():\n",
    "    \"\"\" question 1 \"\"\"\n",
    "    print(\"Start question 1!\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    batch_size = 128\n",
    "    epoch = 10\n",
    "\n",
    "    learning_rate = 0.05\n",
    "    decay = 0.0001\n",
    "\n",
    "    data_collector = DataCollector(\"../data/mnist.pkl\")\n",
    "    graph_visualizer = DataVisualization()\n",
    "\n",
    "    trX, trY = data_collector.get_train_data()\n",
    "    teX, teY = data_collector.get_test_data()\n",
    "\n",
    "    trX = trX.reshape(-1, 1, 28, 28)\n",
    "    teX = teX.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    trX, trY = trX[:12000], trY[:12000]\n",
    "    teX, teY = teX[:2000], teY[:2000]\n",
    "\n",
    "    \"\"\" \n",
    "    Question 1: SGD\n",
    "    \"\"\"\n",
    "    cnn = SoftmaxCNN()\n",
    "    cnn.init_learning_sgd(learning_rate, decay)\n",
    "    cnn.create_model(\n",
    "        convolutional_layer=[(15, 1, 9, 9), (20, 15, 5, 5)],\n",
    "        pooling_layer=[(2, 2), (2, 2)],\n",
    "        hidden_layer=[(20*3*3, 100)],\n",
    "        num_output=10)\n",
    "    accuracy, costs = cnn.start_train(trX, trY, teX, teY, epoch, batch_size)\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=accuracy,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Mean accuracy\",\n",
    "        title=\"Accuracy\",\n",
    "        figure_name=\"../data/Figure1.1-AccuracyEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=costs,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Cross Entropy Cost\",\n",
    "        title=\"Training Cost\",\n",
    "        figure_name=\"../data/Figure1.2-CostEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "\n",
    "    ind = np.random.randint(low=0, high=2000)\n",
    "    outputs = cnn.test(teX[ind:ind+1,:])\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[0], figure_name=\"../data/Figure1.3.1-ConvLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=20,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[1], figure_name=\"../data/Figure1.3.3-ConvLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=6,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[2], figure_name=\"../data/Figure1.3.2-PoolLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=10,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[3], figure_name=\"../data/Figure1.3.4-PoolLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=3,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "\n",
    "    print(\"Finished question 1\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Repeat part 1 by adding the momentum term to mini batch gradient descent learning with momentum parameter (gamma) = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Start script question 2\n",
    "\"\"\" \n",
    "\n",
    "# import numpy as np\n",
    "# from project_a.cnn_softmax import SoftmaxCNN\n",
    "# from project_a.data_collector import DataCollector\n",
    "# from project_a.data_visualization import DataVisualization\n",
    "\n",
    "def question2():\n",
    "    \"\"\" question 2 \"\"\"\n",
    "    print(\"Start question 2!\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    batch_size = 128\n",
    "    epoch = 10\n",
    "\n",
    "    learning_rate = 0.05\n",
    "    decay = 0.0001\n",
    "    momentum = 0.1\n",
    "\n",
    "    data_collector = DataCollector(\"../data/mnist.pkl\")\n",
    "    graph_visualizer = DataVisualization()\n",
    "\n",
    "    trX, trY = data_collector.get_train_data()\n",
    "    teX, teY = data_collector.get_test_data()\n",
    "\n",
    "    trX = trX.reshape(-1, 1, 28, 28)\n",
    "    teX = teX.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    trX, trY = trX[:12000], trY[:12000]\n",
    "    teX, teY = teX[:2000], teY[:2000]\n",
    "\n",
    "    \"\"\"\n",
    "    Question 2: SGD with momentum\n",
    "    \"\"\"\n",
    "    cnn2 = SoftmaxCNN()\n",
    "    cnn2.init_learning_sgd(learning_rate, decay, momentum)\n",
    "    cnn2.create_model(\n",
    "        convolutional_layer=[(15, 1, 9, 9), (20, 15, 5, 5)],\n",
    "        pooling_layer=[(2, 2), (2, 2)],\n",
    "        hidden_layer=[(20*3*3, 100)],\n",
    "        num_output=10)\n",
    "    accuracy, costs = cnn2.start_train(trX, trY, teX, teY, epoch, batch_size)\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=accuracy,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Mean accuracy\",\n",
    "        title=\"Accuracy\",\n",
    "        figure_name=\"../data/Figure2.1-AccuracyEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=costs,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Cross Entropy Cost\",\n",
    "        title=\"Training Cost\",\n",
    "        figure_name=\"../data/Figure2.2-CostEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "\n",
    "    ind = np.random.randint(low=0, high=2000)\n",
    "    outputs = cnn2.test(teX[ind:ind+1,:])\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[0], figure_name=\"../data/Figure2.3.1-ConvLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=20,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[1], figure_name=\"../data/Figure2.3.3-ConvLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=6,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[2], figure_name=\"../data/Figure2.3.2-PoolLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=10,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[3], figure_name=\"../data/Figure2.3.4-PoolLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=3,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "\n",
    "\n",
    "    print(\"Finished question 2\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Repeat part 1 by using RMSProp algorithm for learning. Use learning rate = 0.001, decay = 0.0001 , rho = 0.9,\n",
    "and epsilon = 0.000001 for RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Start script question 3\n",
    "\"\"\" \n",
    "\n",
    "import numpy as np\n",
    "from project_a.cnn_softmax import SoftmaxCNN\n",
    "from project_a.data_collector import DataCollector\n",
    "from project_a.data_visualization import DataVisualization\n",
    "\n",
    "def question3():\n",
    "    \"\"\" question 3 \"\"\"\n",
    "    print(\"Start question 3!\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    batch_size = 128\n",
    "    epoch = 10\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    decay = 0.0001\n",
    "    rho = 0.9\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    data_collector = DataCollector(\"../data/mnist.pkl\")\n",
    "    graph_visualizer = DataVisualization()\n",
    "\n",
    "    trX, trY = data_collector.get_train_data()\n",
    "    teX, teY = data_collector.get_test_data()\n",
    "\n",
    "    trX = trX.reshape(-1, 1, 28, 28)\n",
    "    teX = teX.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    trX, trY = trX[:12000], trY[:12000]\n",
    "    teX, teY = teX[:2000], teY[:2000]\n",
    "\n",
    "    \"\"\"\n",
    "    Question 3: RPMS Prop\n",
    "    \"\"\"\n",
    "    cnn3 = SoftmaxCNN()\n",
    "    cnn3.init_learning_RMSProp(learning_rate, decay, rho, epsilon)\n",
    "    cnn3.create_model(\n",
    "        convolutional_layer=[(15, 1, 9, 9), (20, 15, 5, 5)],\n",
    "        pooling_layer=[(2, 2), (2, 2)],\n",
    "        hidden_layer=[(20*3*3, 100)],\n",
    "        num_output=10)\n",
    "    accuracy, costs = cnn3.start_train(trX, trY, teX, teY, epoch, batch_size)\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=accuracy,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Mean accuracy\",\n",
    "        title=\"Accuracy\",\n",
    "        figure_name=\"../data/Figure3.1-AccuracyEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "    graph_visualizer.plot_graphs(\n",
    "        list_x_point=range(epoch),\n",
    "        list_y_point=costs,\n",
    "        x_label=\"Epoch\",\n",
    "        y_label=\"Cross Entropy Cost\",\n",
    "        title=\"Training Cost\",\n",
    "        figure_name=\"../data/Figure3.2-CostEpoch.png\",\n",
    "        show_image=False\n",
    "    )\n",
    "\n",
    "    ind = np.random.randint(low=0, high=2000)\n",
    "    outputs = cnn3.test(teX[ind:ind+1,:])\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[0], figure_name=\"../data/Figure2.3.1-ConvLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=20,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[1], figure_name=\"../data/Figure2.3.3-ConvLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=6,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "    graph_visualizer.plot_images(\n",
    "        outputs[2], figure_name=\"../data/Figure2.3.2-PoolLayer1.png\", \n",
    "        number_column=5, limit_image=15, size=10,\n",
    "        show_image=False\n",
    "    )\n",
    "    # graph_visualizer.plot_images(\n",
    "    #     outputs[3], figure_name=\"../data/Figure2.3.4-PoolLayer2.png\", \n",
    "    #     number_column=5, limit_image=20, size=3,\n",
    "    #     show_image=False\n",
    "    # )\n",
    "\n",
    "\n",
    "    print(\"Finished question 3\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question3()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
