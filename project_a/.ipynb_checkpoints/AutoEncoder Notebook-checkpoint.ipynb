{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of assignment aims to provide you with some exposure to the use of autoencoders. Use\n",
    "the full MNIST dataset for this problem.\n",
    "Hints: Use corruption level = 0.1, training epochs = up to about 25, learning rate = 0.1, and batch\n",
    "size = 128 for training of all the layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For an easier implementation we would like to implement most of the function in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "\n",
    "def init_weights(n_in, n_out, name_weight):\n",
    "    weight = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_in + n_out)),\n",
    "            high=4 * np.sqrt(6. / (n_in + n_out)),\n",
    "            size=(n_in, n_out)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=weight, name=name_weight, borrow=True)\n",
    "\n",
    "\n",
    "def init_bias(n, name_bias):\n",
    "    return theano.shared(value=np.zeros(n, dtype=theano.config.floatX), name=name_bias, borrow=True)\n",
    "\n",
    "\n",
    "def init_weight_biases_4dimension(filter_shape, d_type):\n",
    "    fan_in = np.prod(filter_shape[1:])\n",
    "    fan_out = filter_shape[0] * np.prod(filter_shape[2:])\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[0],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def init_weight_biases_2dimensions(filter_shape, d_type):\n",
    "    fan_in = filter_shape[1]\n",
    "    fan_out = filter_shape[0]\n",
    "\n",
    "    bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "    w_values = np.asarray(\n",
    "        np.random.uniform(low=-bound, high=bound, size=filter_shape),\n",
    "        dtype=d_type)\n",
    "    b_values = np.zeros((filter_shape[1],), dtype=d_type)\n",
    "    return theano.shared(w_values, borrow=True), theano.shared(b_values, borrow=True)\n",
    "\n",
    "\n",
    "def shuffle_data(samples, labels):\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAutoEncoder:\n",
    "    def __init__(self, num_features, num_outputs, list_hidden_layer, learning_rate,\n",
    "                 corruption_level=0.01,\n",
    "                 sparsity_parameter=0.01, penalty_parameter=0.01):\n",
    "\n",
    "        \"\"\"\n",
    "        :param list_hidden_layer: [10, 20] -> means 2 hidden layer, 10 neurons ->layer_1, 10 neurons -> layer_2\n",
    "        :param corruption_level: made the noise in the data\n",
    "        :param learning_rate: learning rate\n",
    "        :param sparsity_parameter: 0.1 -> means 0.1 of neurons is activated\n",
    "        :param penalty_parameter: learning decay\n",
    "        \"\"\"\n",
    "\n",
    "        list_neurons = [num_features] + list_hidden_layer\n",
    "        x = T.matrix('x')\n",
    "        d = T.matrix('d')\n",
    "\n",
    "        self.total_costs_auto_encoder = []\n",
    "        self.total_costs_full = []\n",
    "        self.total_predictions_full = []\n",
    "\n",
    "        \"\"\"\n",
    "            Do the data corruption\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.corrupt_the_data(corruption_level=corruption_level, x=x)\n",
    "\n",
    "        \"\"\"\n",
    "            Construct the auto encoder\n",
    "        \"\"\"\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        prev_output = x\n",
    "\n",
    "        \"\"\"\n",
    "            ENCODER\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(1, len(list_neurons)):\n",
    "\n",
    "            weight = init_weights(list_neurons[i-1], list_neurons[i], 'weight_%s' % i)\n",
    "            bias = init_bias(list_neurons[i], 'bias_%s' % i)\n",
    "\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "\n",
    "            prev_output = T.nnet.sigmoid(T.dot(prev_output, weight) + bias)\n",
    "\n",
    "        \"\"\"\n",
    "            DECODER\n",
    "        \"\"\"\n",
    "\n",
    "        buffer_output = prev_output\n",
    "        biases_trans = []\n",
    "\n",
    "        for i in range(len(weights)-1, -1, -1):\n",
    "\n",
    "            weight_transpose = weights[i].transpose()\n",
    "            bias_transpose = init_bias(list_neurons[i], 'bias_trans_%s' % i)\n",
    "\n",
    "            biases_trans.append(bias_transpose)\n",
    "\n",
    "            prev_output = T.nnet.sigmoid(T.dot(prev_output, weight_transpose) + bias_transpose)\n",
    "\n",
    "        cost = - T.mean(T.sum(x * T.log(prev_output) + (1 - x) * T.log(1 - prev_output), axis=1))\n",
    "\n",
    "        params = weights+biases+biases_trans\n",
    "        grads = T.grad(cost, params)\n",
    "        updates = [(param, param - learning_rate*grad) for param, grad in zip(params, grads)]\n",
    "\n",
    "        self.train_encoder = theano.function(\n",
    "            inputs=[x],\n",
    "            updates=updates,\n",
    "            outputs=[prev_output, cost]\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "            TRAIN THE FULL CONNECTED LAYER\n",
    "        \"\"\"\n",
    "\n",
    "        last_weight = init_weights(list_neurons[-1], num_outputs, 'last_weight')\n",
    "        last_bias = init_bias(num_outputs, 'last_bias')\n",
    "\n",
    "        buffer_output = T.nnet.softmax(T.dot(buffer_output, last_weight) + last_bias)\n",
    "        y_pred = T.argmax(buffer_output, axis=1)\n",
    "\n",
    "        cost_cross = T.mean(T.nnet.categorical_crossentropy(buffer_output, d))\n",
    "\n",
    "        params_full = weights + [last_weight] + biases + [last_bias]\n",
    "        grads_full = T.grad(cost_cross, params_full)\n",
    "        updates_full = [(param, param - learning_rate * grad ) for param, grad in zip(params_full, grads_full)]\n",
    "\n",
    "        self.train_cross = theano.function(\n",
    "            inputs=[x, d],\n",
    "            updates=updates_full,\n",
    "            outputs=[prev_output, y_pred, cost_cross]\n",
    "        )\n",
    "\n",
    "        self.test_cross = theano.function(\n",
    "            inputs=[x],\n",
    "            outputs=[y_pred]\n",
    "        )\n",
    "\n",
    "    def corrupt_the_data(self, corruption_level, x):\n",
    "\n",
    "        # use binomial dist at corrupt the data\n",
    "        rng = np.random.RandomState(123)\n",
    "        theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "        tilde_x = theano_rng.binomial(size=x.shape, n=1, p=1 - corruption_level,\n",
    "                                      dtype=theano.config.floatX) * x\n",
    "\n",
    "        return tilde_x\n",
    "\n",
    "    def start_train_auto_encoder(self, epochs, batch_size, train_x, train_y, verbose=False):\n",
    "\n",
    "        print \"Start training the auto encoder\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # go through trainng set\n",
    "\n",
    "            costs = []\n",
    "            results = []\n",
    "\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_y), batch_size)):\n",
    "                result, cost = self.train_encoder(train_x[start:end])\n",
    "                costs.append(cost)\n",
    "                results.append(result)\n",
    "\n",
    "            self.total_costs_auto_encoder.append(np.mean(costs, dtype='float64'))\n",
    "\n",
    "            if epoch % 100 == 0 and verbose:\n",
    "                print \"Epoch: %d Cost: %s \\n\" % (epoch, self.total_costs_auto_encoder[epoch])\n",
    "\n",
    "    def start_train_the_full(self, epochs, batch_size, train_x, train_y, test_x, test_y):\n",
    "\n",
    "        print \"Start training the full hidden layer with autoencoder\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # go through trainng set\n",
    "            costs = []\n",
    "            results = []\n",
    "\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_y), batch_size)):\n",
    "                output, result, cost = self.train_cross(train_x[start:end], train_y[start:end])\n",
    "                costs.append(cost)\n",
    "                results.append(np.mean(np.argmax(test_y, axis=1) == self.test_cross(test_x)))\n",
    "\n",
    "            self.total_costs_full.append(np.mean(costs, dtype='float64'))\n",
    "            self.total_predictions_full.append(np.mean(results, dtype='float64'))\n",
    "            print \"result: %s, cost: %s \\n\" % (self.total_costs_full[epoch], self.total_predictions_full[epoch])\n",
    "\n",
    "    def get_total_costs_of_auto_encoder(self):\n",
    "\n",
    "        return self.total_costs_auto_encoder\n",
    "\n",
    "    def get_total_cost_and_prediction_full(self):\n",
    "\n",
    "        return self.total_costs_full, self.total_predictions_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collections class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "class DataCollector:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "\n",
    "        with open(file_path, \"rb\") as input_file:\n",
    "            self.data = cPickle.load(input_file)\n",
    "\n",
    "        self.data_train = self.data[0]\n",
    "        self.data_test = self.data[2]\n",
    "        self.data_validate = self.data[1]\n",
    "\n",
    "    def get_train_data(self):\n",
    "\n",
    "        return self.data_train[0], self.return_one_hot_encoding(10, self.data_train[1])\n",
    "\n",
    "    def get_test_data(self):\n",
    "\n",
    "        return self.data_test[0], self.return_one_hot_encoding(10, self.data_test[1])\n",
    "\n",
    "    def get_validation_data(self):\n",
    "\n",
    "        return self.data_validate[0], self.return_one_hot_encoding(10, self.data_validate[1])\n",
    "\n",
    "    def return_one_hot_encoding(self, num_output, list_data):\n",
    "\n",
    "        zeros = np.zeros((len(list_data), num_output))\n",
    "\n",
    "        for i in range(len(zeros)):\n",
    "\n",
    "            zeros[i][list_data[i]] = 1\n",
    "\n",
    "        return zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataVisualization:\n",
    "\n",
    "   def __init__(self):\n",
    "       return\n",
    "\n",
    "   def show_plot(self, list_x_point, list_y_point, x_label, y_label, title, figure_name):\n",
    "       plt.figure()\n",
    "       plt.plot(list_x_point, list_y_point)\n",
    "       plt.xlabel(x_label)\n",
    "       plt.ylabel(y_label)\n",
    "       plt.title(title)\n",
    "       plt.savefig(figure_name)\n",
    "       plt.show()\n",
    "\n",
    "\n",
    "class DataVisualizationWithLabels:\n",
    "   def __init__(self):\n",
    "       return\n",
    "\n",
    "   def show_plot(self, list_x_point, list_y_point, x_label, y_label, title, figure_name, labels):\n",
    "       plt.figure()\n",
    "\n",
    "       for cnt in range(len(labels)):\n",
    "           plt.plot(list_x_point[cnt], list_y_point[cnt], label=labels[cnt])\n",
    "\n",
    "       plt.xlabel(x_label)\n",
    "       plt.ylabel(y_label)\n",
    "       plt.title(title)\n",
    "       plt.legend()\n",
    "       plt.savefig(figure_name)\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollector(\"../mnist.pkl\")\n",
    "data_visualize = DataVisualizationWithLabels()\n",
    "\n",
    "train_x, train_y = data_collector.get_train_data()\n",
    "test_x, test_y = data_collector.get_test_data()\n",
    "validate_x, validate_y = data_collector.get_validation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Design a stacked denoising autoencoder consisting of three hidden-layers; 900 neurons in the\n",
    "first hidden-layer, 625 neurons in the second hidden-layer, and 400 neurons in the third\n",
    "hidden-layer. To train the network:\n",
    "- Use the training dataset of MNIST digits\n",
    "- Corrupt the input data using a binomial distribution at 10% corruption level.\n",
    "- Use cross-entropy as the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\r\n",
      "-rw-r--r--  1 edwardsujono  staff  22524 Oct 27 18:05 AutoEncoder Notebook.ipynb\r\n",
      "-rw-r--r--  1 edwardsujono  staff      0 Oct 25 22:12 __init__.py\r\n",
      "-rw-r--r--  1 edwardsujono  staff   3844 Oct 27 02:35 softmax_with_cnn.py\r\n",
      "-rw-r--r--  1 edwardsujono  staff   4073 Oct 27 02:42 softmax_with_cnn.pyc\r\n",
      "-rw-r--r--  1 edwardsujono  staff    811 Oct 27 02:42 start.py\r\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the auto encoder\n",
      "Epoch: 0 Cost: 546.92141014 \n",
      "\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './images/1_cost.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d599336d998c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cross entropy costs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfigure_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-23-613de9fc8c31>\u001b[0m in \u001b[0;36mshow_plot\u001b[0;34m(self, list_x_point, list_y_point, x_label, y_label, title, figure_name, labels)\u001b[0m\n\u001b[1;32m     31\u001b[0m        \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m        \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m        \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m        \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/edwardsujono/Python_Project/assignment_neural_network_2/ENV/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/edwardsujono/Python_Project/assignment_neural_network_2/ENV/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/edwardsujono/Python_Project/assignment_neural_network_2/ENV/lib/python2.7/site-packages/matplotlib/backend_bases.pyc\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2250\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2253\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/edwardsujono/Python_Project/assignment_neural_network_2/ENV/lib/python2.7/site-packages/matplotlib/backends/backend_agg.pyc\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './images/1_cost.png'"
     ]
    }
   ],
   "source": [
    "num_feature = len(train_x[0])\n",
    "epochs = 1\n",
    "batch_size = 20000\n",
    "file_save = './images/1_cost.png'\n",
    "\n",
    "softmax = SoftmaxAutoEncoder(num_features=num_feature, num_outputs=10,\n",
    "                             list_hidden_layer=[10], learning_rate=0.05,\n",
    "                            corruption_level=0.01)\n",
    "\n",
    "softmax.start_train_auto_encoder(epochs=epochs, batch_size=batch_size, train_x=train_x, \n",
    "                                 train_y=train_y, verbose=True)\n",
    "\n",
    "total_costs_auto_encoder = softmax.get_total_costs_of_auto_encoder()\n",
    "\n",
    "\n",
    "data_visualize.show_plot(\n",
    "    list_y_point=total_costs_auto_encoder, \n",
    "    list_x_point=range(epochs),\n",
    "    x_label='epochs',\n",
    "    y_label='costs',\n",
    "    title='cross entropy costs',\n",
    "    figure_name=file_save,\n",
    "    labels=['cost']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
